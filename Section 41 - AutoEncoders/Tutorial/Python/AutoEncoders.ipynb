{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi guys,\n",
    "\n",
    "we will be working on the same dataset as in **Part 5 - Boltzmann Machines** so the Data Preprocessing phase is the same for *Parts 5* and *6*. Therefore, if you already completed *Part 5*, feel free to skip the five following tutorials and jump directly to the Lecture: **Building an AutoEncoder - Step 6**.\n",
    "\n",
    "*Enjoy Deep Learning!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"ml-1m/users.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter = \"\\t\")\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set, dtype = \"int\")\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"ml-100k/u1.test\", delimiter = \"\\t\")\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = np.array(test_set, dtype = \"int\")\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the number of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
    "nb_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Challenge - Coding Exercise\n",
    "\n",
    "So far our training and test sets have the following format:\n",
    "\n",
    "*Column 1:* User\n",
    "\n",
    "*Column 2:* Movie\n",
    "\n",
    "*Column 3:* Rating\n",
    "\n",
    "*Column 4:* Timestamp\n",
    "\n",
    "Define a function that will convert this format into a list of horizontal lists, where each horizontal list corresponds to a user and includes all its ratings of the movies. In each list should also be included the movies that the user didn't rate and for these movies, just put a zero. So what you should get in the end is a huge list of **943** horizontal lists (because there are **943** users):\n",
    "\n",
    "*List of User 1:* `[Ratings of all the movies by User 1]`\n",
    "\n",
    "*List of User 2:* `[Ratings of all the movies by User 2]`\n",
    "\n",
    "................................................................................\n",
    "\n",
    "*List of User 943:* `[Ratings of all the movies by User 943]`\n",
    "\n",
    "Why doing this? Because we want to create a new structure of data, having the shape of a 2d array where:\n",
    "\n",
    "the rows are the users,\n",
    "the columns are the movies,\n",
    "the cells are the ratings.\n",
    "\n",
    "This coding exercise will be excellent practice for you because you will work with four important techniques in Python:\n",
    "\n",
    "functions\n",
    "lists and arrays\n",
    "for loops\n",
    "handling indexes\n",
    "Try to complete this Homework as hard as you can, the more you try, the more you will progress.\n",
    "\n",
    "The solution is in the next tutorial.\n",
    "\n",
    "*Good luck!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into an array with users in lines and movies in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torch.FloatTensor(test_set)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    # The below function basically defines the architecture of the neural network.\n",
    "    def __init__(self, ):\n",
    "        # It will make sure that we get all the inherited classes and methods of the parent class and that module.\n",
    "        super(SAE, self).__init__()\n",
    "\n",
    "        # The below line represents the full connection between the first input vector features and the first encoded vector.\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # (number of features in the input vector, No. of Nodes in First Hidden Layer)\n",
    "        self.fc2 = nn.Linear(20, 10) # (No. of Nodes in First Hidden Layer, No. of Nodes in Second Hidden Layer)\n",
    "        self.fc3 = nn.Linear(10, 20) # (No. of Nodes in Second Hidden Layer, No. of Nodes in Third Hidden Layer)\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # (No. of Nodes in Third Hidden Layer, No. of Nodes in Output Layer and \"output_vec = input_vec\")\n",
    "\n",
    "        # Specifying the Activation Function\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    # Function for different encodings and decodings when the observation is forwarded into the network. It will also apply to different activation functions inside the full connections.\n",
    "    # The main purpose of making this function is that it will return in the end the vector of predicted ratings that we will compare to the vector of real ratings, that is, the input vector.\n",
    "    def forward(self, x): # \"x\" -> input vector\n",
    "        # Encoding the input vector feature i.e., \"x\" into a first shorter vector composed of 20 elements in our first hidden layer.\n",
    "\n",
    "        # This is the new first encoded vector resulting from the first encoding that happens here with the AF in the first fc.\n",
    "        x = self.activation(self.fc1(x))\n",
    "\n",
    "        # Doing same above thing for the other full connections\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x) # Here it's the output layer that's why we don't use the activation function (AF) here.\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the object of SAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "\n",
    "# Criterion for the Loss Function and the Loss Function is going to be the Mean Squared Error.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Creating an Optimizer\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)    # (all the parameters of our AutoEncoders, Learning Rate, Decay which is used to reduce the LR after every few epochs and that's in order to regulate the convergence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of epochs\n",
    "nb_epochs = 200\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.  # Counter\n",
    "\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        # Cloning the input variable\n",
    "        target = input.clone()\n",
    "\n",
    "        # The purpose of doing this is to optimize the memory. To save as much memory as possible because this if condition will consist of only looking at the users who rated at least one movie.\n",
    "        # \"target.data\" is all the ratings. But we have to consider all the ratings that are larger than zero.\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            # Vector of predicted ratings\n",
    "            output = sae(input)\n",
    "            target.requires_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0 + 1e-10))\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"epoch: \" + str(epoch) + \" loss: \" + str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.requires_grad = False\n",
    "        target = target.unsqueeze(0)  # Add a dimension at index 0\n",
    "        mask = target == 0\n",
    "        output[mask] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0 + 1e-10))\n",
    "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "        s += 1.\n",
    "\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
